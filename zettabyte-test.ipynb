{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zettabyte Technical Test\n",
    "\n",
    "Name: Daffa Haj Tsaqif\n",
    "\n",
    "Note: This code uses python 3.7.14 to make it easy to run it in Google Colab due to Colab still uses that version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dhupee/zettabyte-test/blob/master/zettabyte-test.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.14\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n",
    "COLAB_MODE = False # Set to True if you use Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "1. What are the different domains of AI?\n",
    "2. What is the relation between Machine Learning and Artificial Intelligence?\n",
    "3. What are the different types of Machine Learning in AI? Explain and give examples of each.\n",
    "4. Do Text Processing with the given text\n",
    "5. Make simple face recognition with Haar Cascade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Answer\n",
    "\n",
    "### 1. Different Domains of AI\n",
    "\n",
    "#### Artificial Intelligence\n",
    "The term \"Domain\" of AI is very broad, so I decided to use the image below as the case of Domains of AI.\n",
    "Artificial Intelligence is a broad term for any intelligence that can be demonstrated by machines, where machines can do tasks that previously need human intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Domains](img/Internship-Robby-Cabang-AI.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence has a few subsets, some of them are:\n",
    "* Machine Learning\n",
    "* Natural Language Processing\n",
    "* Computer Vision\n",
    "* Deep Leaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning\n",
    "\n",
    "Machine Learning is a subset of Artificial Intelligence then the machine learning pattern of the given datasets using an algorithm, each algorithm has specialized tasks such as predicting future values, detecting anomalies, etc.\n",
    "\n",
    "Scikit Learn is one of the known Machine Learning libraries in Python\n",
    "\n",
    "In this area of Machine Learning where these 3 terms are introduced:\n",
    "* Supervised Learning\n",
    "* Unsupervised Learning\n",
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing\n",
    "\n",
    "In the diagram above Speech and NLP is considered separate area, but I will explain them as one.\n",
    "\n",
    "Natural Language Processing is a subfield of AI that provides computers to understand and process human language like English, or Bahasa. A few known application of NLP is virtual assistant like Google Home or Siri, Auto CC on Youtube is also part of the NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linus AutoCC](img/Screenshot%20from%202022-09-19%2021-07-01.png)\n",
    "\n",
    "An example of NLP is Auto CC on Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer Vision\n",
    "\n",
    "Computer vision is a subfield of AI that provides computers the ability to process received images and video files or live feeds.\n",
    "\n",
    "Computer vision is widely used to do image recognition, Optical Character Recognition(OCR) to extract text from imagery, self-driving vehicles, etc.\n",
    "\n",
    "OpenCV, MediaPipe, and EasyOCR are one of the few CV libraries in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FS Driverless Car](img/FSG-MIT-Delft-Driverless-Car-Competition-00_0.jpg)\n",
    "\n",
    "Example of Computer Vision Application, Formula Student Driverless Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning\n",
    "\n",
    "Deep Learning is a subset of Machine Learning where the core purpose is the same which is to learn a pattern from a given dataset but instead of using algorithms such as Regression or K-Means, Deep Learning uses *Artificial Neural Network* or ANN which is interconnected node-based algorithms that are based on animal and human brains.\n",
    "\n",
    "Deep Learning has a few architectures, such as:\n",
    "* Convolutional Neural Network(CNN)\n",
    "* Recurrent Neural Network(RNN)\n",
    "* Long-Short Term Memory(LSTM)\n",
    "\n",
    "Tensorflow and PyTorch are the 2 most famous Deep Learning libraries out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning](img/0_Q2z_Dz8rOOS874QL.png)\n",
    "\n",
    "Example of Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relation between ML and AI\n",
    "\n",
    "AI itself is a broad term for Machines that are capable to do tasks that usually require human intelligence whereas Machine Learning is a subset of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AI-ML Relationship](img/0_HUsFDWZ_NBSsIie5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Different Types of ML with Example\n",
    "\n",
    "In the previous explanation, I stated that Machine Learning usually divided into 3 categories/types, which are:\n",
    "* Supervised Learning\n",
    "* Unsupervised Learning\n",
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "Supervised Learning is a type of machine learning where the data that is being fed to the Machine Learning model has a certain label or category, which the model will learn what pattern each label will have.\n",
    "\n",
    "Some of the use for this machine learning are:\n",
    "* Discord Bot learning whether the chat in the server is toxic or not\n",
    "* Image recognition to differentiate different animals\n",
    "* Recommendations of a new movie that users might like it\n",
    "* Weather Forecast to predict how the weather will behave on a given day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "\n",
    "Unsupervised Learning is a type of machine learning where the model is only fed with data that doesn't have any label or categories, so the model might find any hidden pattern.\n",
    "\n",
    "Use cases of this method are:\n",
    "* Clustering\n",
    "* Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a type of ML where the model learns by _Trial_ and Error* over time in an environment to maximize a reward given by the programmer. The way the model learn is similar to teaching a pet to do tricks then when the pet to something right it will be given a treat or reward.\n",
    "\n",
    "Use case of RL:\n",
    "* Humanoid Robot trains itself to stand up\n",
    "* self-driving car simulation teaches itself to park on a designated spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Processing\n",
    "\n",
    "Note: at the time of writing this submission, my question about the detail of the task hasn't been answered yet so I will do some fun stuff but still related to text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is only useful if used in Colab\n",
    "\n",
    "if COLAB_MODE == True:\n",
    "    r_link = 'https://raw.githubusercontent.com/dhupee/zettabyte-test/master/requirements.txt'\n",
    "    r_file = 'requirements.txt'\n",
    "\n",
    "    t_link = 'https://raw.githubusercontent.com/dhupee/zettabyte-test/master/text.txt'\n",
    "    t_file = 'text.txt'\n",
    "\n",
    "    import urllib\n",
    "    # download requirements.txt from GitHub then install dependencies\n",
    "    urllib.request.urlretrieve(r_link, r_file)\n",
    "    %pip install -r 'requirements.txt'\n",
    "\n",
    "    urllib.request.urlretrieve(t_link, t_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Word Token in the Text File with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# uncomment the line below if there is lookup error\n",
    "# nltk.download('popular')\n",
    "\n",
    "text_file = 'text.txt'\n",
    "\n",
    "def process_txt(text_file):\n",
    "    punctuations = '''!()-[]{};:'`\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    with open(text_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        # getting tokens of the txt file\n",
    "        tokens = nltk.word_tokenize(data)\n",
    "\n",
    "    # lower cased all tokens then delete duplicates\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "        tokens = list(dict.fromkeys(tokens))\n",
    "    \n",
    "    # remove punctuations, don't need it anyway\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in punctuations:\n",
    "                token = token.replace(token, \"\")\n",
    "            new_tokens.append(token)\n",
    "        tokens = new_tokens\n",
    "\n",
    "        print('there are ' + str(len(tokens)) + ' tokens')\n",
    "\n",
    "try:\n",
    "    process_txt(text_file)\n",
    "except LookupError: # to be honest, I haven't tried this exception\n",
    "    nltk.download('popular')\n",
    "    process_txt(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add line spaces in the text file\n",
    "\n",
    "the text that is given has no line space, so I have a hard time tried to read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of Chapter One is 'A Stop on the Salt Route'\n",
      "The story took place in 1000 B.C.\n",
      "\n",
      "line space has been added in the new file\n"
     ]
    }
   ],
   "source": [
    "new_file = 'new_text.txt' # new file name\n",
    "\n",
    "# open the file\n",
    "with open(text_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = data.split('\\n')\n",
    "\n",
    "chapter = data[0]\n",
    "title = data[1]\n",
    "time = data[2]\n",
    "print('The title of '+ chapter + \" is '\" + title + \"'\")\n",
    "print('The story took place in ' + time)\n",
    "\n",
    "# add line space then write to the new file\n",
    "with open(new_file, 'w') as w:\n",
    "    for i in range(len(data)):\n",
    "        data[i] += '\\n \\n'\n",
    "        w.write(data[i])\n",
    "    print('\\nline space has been added in the new file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate text into Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translated_file = 'id_text.txt'\n",
    "with open(new_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = data.split('\\n')\n",
    "    with open(translated_file, 'w') as w:\n",
    "        for i in range(len(data)):\n",
    "            data[i] = GoogleTranslator(source='en', target='id').translate(data[i])\n",
    "            w.write(data[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Simple Face Recognition\n",
    "\n",
    "note: hit q to close, also I add Haar Cascade for eye too (for myself to learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing these 2 will be kinda long, hold it\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# https://pypi.org/project/opencv-python/ for further reading, I need it\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect my face\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        # add rectangle in my face\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        # detect my eyes too\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    # check if I hit 'q' then will break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the capture\n",
    "cap.release()\n",
    "# frame will be closed\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Result\n",
    "![My Ugly Face](img/Screenshot%20from%202022-09-21%2003-00-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't know whether the additional 'eye' rectangle is cause by my bad code or my low quality webcam.\n",
    "\n",
    "Sometimes it will false capture my shirt too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "985f6a1ed89e05fdace84f186ab2a0956ccaa75c9f84e2b84fa607aba79461ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
